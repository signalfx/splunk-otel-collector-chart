# Configurable parameters and default values for splunk-otel-collector.
# This is a YAML-formatted file.
# Declared variables will be passed into templates.

################################################################################
# clusterName is a REQUIRED field. It can be set to an arbitrary value
# that identifies this K8s cluster in SignalFx. The value will be associated
# with every trace, metric and log as "k8s.cluster.name" metadata attribute.
################################################################################

clusterName: k8s-cluster


################################################################################
# Splunk SignalFx backend configuration
################################################################################

# The SignalFx realm to send telemetry data to. If set, the values of `ingestUrl`
# and `apiUrl` will be automatically set based on this
# realm value. If not set, it defaults to the original "us0" realm.
splunkRealm: us0

# (REQUIRED) SignalFx org access token.
splunkAccessToken:

# SignalFx ingest server host, default: "ingest.<realm>.signalfx.com".
ingestHost:

# The SignalFx API URL, default: "htpps://api.<realm>.signalfx.com".
apiUrl:

# Signalfx ingest endpoint port.
ingestPort: 443

# Signalfx ingest protocol: "http" and "https".
ingestProtocol: https

################################################################################
# Kubernetes platform used to run the collector on. Valid options are:
# - "aws" (Amazon EKS and self-managed k8s cluster on AWS EC2)
# - "gcp" (Google GKE and self-managed k8s cluster on GCP GCE)
# - "default"
################################################################################

platform: default

################################################################################
# Telemetry configuration.
# By default metrics, traces and logs are collected from the k8s cluster.
# It's possible to disable any kind of telemetry, if it's not needed.
################################################################################

metricsEnabled: true
tracesEnabled: true
logsEnabled: true

# Configuration for additional metadata that will be added to all the telemetry
# as extra attributes.
extraAttributes:

  # Labels that will be collected from k8s pods (in case they are set)
  # and added  as extra attributes to the telemetry in the following format:
  # k8s.pod.labels.<label_name>: <label_value>
  podLabels: []
    # - app
    # - k8s-app
    # - release

  # List of hardcoded key/value pairs that will be added as attributes to
  # all the telemetry.
  custom: []
    # - name: "account_id"
    #   value: "1234567890"

################################################################################
# OPTIONAL CONFIGURATIONS OF PARTICULAR O11Y COLLECTOR COMPONENTS
################################################################################

################################################################################
# Open-telemetry collector running as an deamonset agent on every node.
# It collects metrics and traces and send them to Signalfx backend.
################################################################################

otelAgent:
  enabled: true

  # The ports to be exposed by the agent to the host.
  # Make sure that only necessary ports are exposed, <hostIP, hostPort, protocol> combination must
  # be unique across all the nodes in k8s cluster. Any port can be disabled,
  # For example to disable zipkin ports set `otelAgent.ports.zipkin: null`.
  ports:
    otlp:
      containerPort: 55680
      hostPort: 55680
      protocol: TCP
    zipkin:
      containerPort: 9411
      hostPort: 9411
      protocol: TCP
    jaeger-thrift:
      containerPort: 14268
      hostPort: 14268
      protocol: TCP
    jaeger-grpc:
      containerPort: 14250
      hostPort: 14250
      protocol: TCP
    fluentforward:
      containerPort: 8006
      hostPort: 8006
      protocol: TCP
    # opencensus:
    #   containerPort: 55678
    #   hostPort: 55678
    #   protocol: TCP

  resources:
    limits:
      cpu: 200m
      # This value is being used as a source for default memory_limiter processor configurations
      memory: 500Mi

  securityContext: {}

  # OpenTelemetry Collector configuration for otel-agent daemonset can be overriden in this field.
  # Default configuration defined in config/otel-agent-config.yaml
  # Any additional fields will be merged into the defaults,
  # existing fields can be disabled by setting them to null value.
  config: {}

################################################################################
# OpenTelemetry Kubernetes cluster receiver
# This is an extra 1-replica deployment of Open-temlemetry collector used
# specifically for collecting metrics from kubernetes API.
################################################################################

# Kubernetes cluster receiver collects cluster level metrics from the Kubernetes API.
# It has to be running on one pod, so it uses its own dedicated deployment with 1 replica.

otelK8sClusterReceiver:
  enabled: true

  # Need to be adjusted based on size of the monitored cluster
  resources:
    limits:
      cpu: 200m
      memory: 500Mi

  # Scheduling configurations
  nodeSelector: {}
  tolerations: []
  affinity: {}

  # Pod configurations
  securityContext: {}
  terminationGracePeriodSeconds: 600
  priorityClassName: ""

  # OpenTelemetry Collector configuration for K8s Cluster Receiver deployment can be overriden in this field.
  # Defaul configuration defined in config/otel-k8s-cluster-receiver-config.yaml
  # Any additional fields will be merged into the defaults,
  # existing fields can be disabled by setting them to null value.
  config: {}

################################################################################
# Fluentd configuration for logs collection
################################################################################

fluentd:
  resources:
    limits:
      cpu: 500m
      memory: 500Mi
    requests:
      cpu: 100m
      memory: 200Mi

  # Currently fluentd logging component need to be run in privileged mode.
  securityContext:
    privileged: true
    runAsUser: 0

  config:
    # Configurations for container logs
    containers:
      # Path to root directory of container logs
      path: /var/log
      # Final volume destination of container log symlinks
      pathDest: /var/lib/docker/containers
      # Log format type, "json" or "cri"
      logFormatType: json
      # Specify the log format for "cri" logFormatType
      # It can be "%Y-%m-%dT%H:%M:%S.%N%:z" for openshift and "%Y-%m-%dT%H:%M:%S.%NZ" for IBM IKS
      logFormat:

    # Directory where to read journald logs. (docker daemon logs, kubelet logs, and anyother specified serivce logs)
    journalLogPath: /run/log/journal

    # Controls the output buffer for the fluentd daemonset
    # Note that, for memory buffer, if `resources.limits.memory` is set,
    # the total buffer size should not bigger than the memory limit, it should also
    # consider the basic memory usage by fluentd itself.
    # All buffer parameters (except Argument) defined in
    # https://docs.fluentd.org/v1.0/articles/buffer-section#parameters
    # can be configured here.
    buffer:
      "@type": memory
      total_limit_size: 600m
      chunk_limit_size: 1m
      chunk_limit_records: 100000
      flush_interval: 5s
      flush_thread_count: 1
      overflow_action: block
      retry_max_times: 3

    # logLevel is to set log level of the Splunk log collector.
    # Available values are: trace, debug, info, warn, error
    logLevel: info

    # path of logfiles, default /var/log/containers/*.log
    path: /var/log/containers/*.log
    # paths of logfiles to exclude. object type is array as per fluentd specification:
    # https://docs.fluentd.org/input/tail#exclude_path
    excludePath:
    #  - /var/log/containers/kube-svc-redirect*.log
    #  - /var/log/containers/tiller*.log

    # Prefix for pos_file tail source parameter
    # Can be used if you want to run multiple instances of fluentd on the same host
    # https://docs.fluentd.org/input/tail#pos_file-highly-recommended
    posFilePrefix: /var/log/splunk-fluentd

    # `customFilters` defines the custom filters to be used.
    # This section can be used to define custom filters using plugins like https://github.com/splunk/fluent-plugin-jq
    # Its also possible to use other filters like https://www.fluentd.org/plugins#filter
    #
    # The scheme to define a custom filter is:
    #
    # ```
    # <name>:
    #   tag: <fluentd tag for the filter>
    #   type: <fluentd filter type>
    #   body: <definition of the fluentd filter>
    # ```
    #
    # = fluentd tag for the filter =
    # This is the fluentd tag for the record
    #
    # = fluentd filter type =
    # This is the fluentd filter that the user wants to use for record manipulation.
    #
    # = definition of the fluentd filter =
    # This defines the body/logic for using the filter for record manipulation.
    #
    # For example if you want to define a filter which sets cluster_name field to "my_awesome_cluster" you would the following filter
    # <filter tail.containers.**>
    #  @type jq_transformer
    #  jq '.record.cluster_name = "my_awesome_cluster" | .record'
    # </filter>
    # This can be defined in the customFilters section as follows:
    # ```
    # customFilters:
    #   NamespaceSourcetypeFilter:
    #     tag: tail.containers.**
    #     type: jq_transformer
    #     body: jq '.record.cluster_name = "my_awesome_cluster" | .record'
    # ```
    customFilters: {}

    #
    # You can find more information on indexed fields here - http://dev.splunk.com/view/event-collector/SP-CAAAFB6
    # The scheme to define an indexed field is:
    #
    # ```
    # ["field_1", "field_2"]
    # ```
    #
    # `indexFields` defines the fields from the fluentd record to be indexed.
    # You can find more information on indexed fields here - http://dev.splunk.com/view/event-collector/SP-CAAAFB6
    # The input is in the form of an array(comma separated list) of the values you want to use as indexed fields.
    #
    # For example if you want to define indexed fields for "field_1" and "field_2"
    # you will have to define an indexFields section as follows in values.yaml file.
    # ```
    # indexFields: ["field_1", "field_2"]
    # ```
    # WARNING: The fields being used here must be available inside the fluentd record.
    indexFields: []

    # `logs` defines the source of logs, multiline support, and their sourcetypes.
    #
    # The scheme to define a log is:
    #
    # ```
    # <name>:
    #   from:
    #     <source>
    #   timestampExtraction:
    #     regexp: "<regexp_to_extract_timestamp_from_log>"
    #     format: "<format_of_the_timestamp>"
    #   multiline:
    #     firstline: "<regexp_to_detect_firstline_of_multiline>"
    #     flushInterval 5s
    #   sourcetype: "<sourcetype_of_logs>"
    # ```
    #
    # = <source> =
    # It supports 3 kinds of sources: journald, file, and container.
    # For `journald` logs, `unit` is required for filtering using _SYSTEMD_UNIT, example:
    # ```
    # docker:
    #   from:
    #     journald:
    #       unit: docker.service
    # ```
    #
    # For `file` logs, `path` is required for specifying where is the log files. Log files are expected in `/var/log`, example:
    # ```
    # docker:
    #   from:
    #     file:
    #       path: /var/log/docker.log
    # ```
    #
    # For `container` logs, pod name is required. You can also provide the container name, if it's not provided, the name of this source will be used as the container name:
    # ```
    # kube-apiserver:
    #   from:
    #     pod: kube-apiserver
    #
    # etcd:
    #   from:
    #     pod: etcd-server
    #     container: etcd-container
    # ```
    #
    # = timestamp =
    # `timestampExtraction` defines how to extract timestamp from logs. This *only* works for `file` source.
    # To use `timestampExtraction` you need to define both:
    # - `regexp`: the Regular Expression used to find the timestamp from a log entry.
    #             The timestamp part must be in a `time` named group. E.g.
    #             (?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})
    # - `format`: a format string defintes how to parse the timestamp, e.g. "%Y-%m-%d %H:%M:%S".
    #             More details can be find: http://ruby-doc.org/stdlib-2.5.0/libdoc/time/rdoc/Time.html#method-c-strptime
    #
    # = multiline =
    # `multiline` options provide basic multiline support. Two options:
    # - `firstline`: a Regular Expression used to detect the first line of a multiline log.
    # - `flushInterval`: The interval between data flushes, default value: 5s.
    #
    # = sourcetype =
    # sourcetype of each kind of log can be defined using the `sourcetype` field.
    # If `sourcetype` is not defined, `name` will be used.
    #
    # ---
    # Here we have some default timestampExtraction and multiline settings for kubernetes components.
    # So, usually you just need to redefine the source of those components if necessary.
    logs:
      docker:
        from:
          journald:
            unit: docker.service
        timestampExtraction:
          regexp: time="(?<time>\d{4}-\d{2}-\d{2}T[0-2]\d:[0-5]\d:[0-5]\d.\d{9}Z)"
          format: "%Y-%m-%dT%H:%M:%S.%NZ"
        sourcetype: kube:docker
      kubelet: &glog
        from:
          journald:
            unit: kubelet.service
        timestampExtraction:
          regexp: \w(?<time>[0-1]\d[0-3]\d [^\s]*)
          format: "%m%d %H:%M:%S.%N"
        multiline:
          firstline: /^\w[0-1]\d[0-3]\d/
        sourcetype: kube:kubelet
      etcd:
        from:
          pod: etcd-server
          container: etcd-container
        timestampExtraction:
          regexp: (?<time>\d{4}-\d{2}-\d{2} [0-2]\d:[0-5]\d:[0-5]\d\.\d{6})
          format: "%Y-%m-%d %H:%M:%S.%N"
      etcd-minikube:
        from:
          pod: etcd-minikube
          container: etcd
        timestampExtraction:
          regexp: (?<time>\d{4}-\d{2}-\d{2} [0-2]\d:[0-5]\d:[0-5]\d\.\d{6})
          format: "%Y-%m-%d %H:%M:%S.%N"
      etcd-events:
        from:
          pod: etcd-server-events
          container: etcd-container
        timestampExtraction:
          regexp: (?<time>\d{4}-[0-1]\d-[0-3]\d [0-2]\d:[0-5]\d:[0-5]\d\.\d{6})
          format: "%Y-%m-%d %H:%M:%S.%N"
      kube-apiserver:
        <<: *glog
        from:
          pod: kube-apiserver
        sourcetype: kube:kube-apiserver
      kube-scheduler:
        <<: *glog
        from:
          pod: kube-scheduler
        sourcetype: kube:kube-scheduler
      kube-controller-manager:
        <<: *glog
        from:
          pod: kube-controller-manager
        sourcetype: kube:kube-controller-manager
      kube-proxy:
        <<: *glog
        from:
          pod: kube-proxy
        sourcetype: kube:kube-proxy
      kubedns:
        <<: *glog
        from:
          pod: kube-dns
        sourcetype: kube:kubedns
      dnsmasq:
        <<: *glog
        from:
          pod: kube-dns
        sourcetype: kube:dnsmasq
      dns-sidecar:
        <<: *glog
        from:
          pod: kube-dns
          container: sidecar
        sourcetype: kube:kubedns-sidecar
      dns-controller:
        <<: *glog
        from:
          pod: dns-controller
        sourcetype: kube:dns-controller
      kube-dns-autoscaler:
        <<: *glog
        from:
          pod: kube-dns-autoscaler
          container: autoscaler
        sourcetype: kube:kube-dns-autoscaler
      kube-audit:
        from:
          file:
            path: /var/log/kube-apiserver-audit.log
        timestampExtraction:
          format: "%Y-%m-%dT%H:%M:%SZ"
        sourcetype: kube:apiserver-audit

################################################################################
# Additional configuration for logs backend.
# It allows to configure o11y collector send logs to a Splunk backend,
# instead of Signalfx.
################################################################################

logsBackend:
  # Configurations for HEC (HTTP Event Collector)
  hec:
    # host will be set to `signalfx.ingestUrl` used by default
    host:
    # port to HEC, optional, default 443
    port:
    # token is required, `signalfx.token` used by default
    token:
    # protocol has two options: "http" and "https", default is "https"
    protocol:
    # indexName tells which splunk index to use, this is optional.
    indexName: main
    # insecureSSL is a boolean, it indicates should it allow inscure SSL connection (when protocol is "https"). Default is false.
    insecureSSL: false
    # The PEM-format CA certificate for this client.
    # NOTE: The content of the certificate itself should be used here, not the file path.
    #       The certificate will be stored as a secret in kubernetes.
    clientCert:
    # The private key for this client.
    # NOTE: The content of the key itself should be used here, not the file path.
    #       The key will be stored as a secret in kubernetes.
    clientKey:
    # The PEM-format CA certificate file.
    # NOTE: The content of the file itself should be used here, not the file path.
    #       The file will be stored as a secret in kubernetes.
    caFile:
  # Configurations for Splunk Ingest API which will be used instead of HEC logs backend if logsBackend.splunkIngestAPI.ingestAPIHost is set
  splunkIngestAPI:
    # serviceClientIdentifier is a string, the client identifier is used to make requests to the ingest API with authorization.
    serviceClientIdentifier:
    # serviceClientSecretKey is a string, the client identifier is used to make requests to the ingest API with authorization.
    serviceClientSecretKey:
    # tokenEndpoint is a string, it indicates which endpoint should be used to get the authorization token used to make requests to the ingest API.
    tokenEndpoint:
    # ingestAuthHost is a string, it indicates which url/hostname should be used to make token auth requests to the ingest API.
    ingestAuthHost:
    # ingestAPIHost is a string, it indicates which url/hostname should be used to make requests to the ingest API.
    ingestAPIHost:
    # tenant is a string, it indicates which tenant should be used to make requests to the ingest API.
    tenant:
    # eventsEndpoint is a string, it indicates which endpoint should be used to make requests to the ingest API.
    eventsEndpoint:
    # debugIngestAPI is a boolean, it indicates whether user wants to debug requests and responses to ingest API. Default is false.
    debugIngestAPI:


################################################################################
# Docker image configuration
################################################################################

image:
  # Secrets to attach to the respective serviceaccount to pull docker images
  imagePullSecrets: []

  fluentd:
    # The domain of the registry to pull the fluentd image from
    registry: docker.io
    # The name of the fluentd image to pull
    name: splunk/fluentd-hec
    # The tag of the fluentd image to pull
    tag: 1.2.4
    # The policy that specifies when the user wants the fluentd images to be pulled
    pullPolicy: IfNotPresent

  otelcol:
    # The domain of the registry to pull the opentelemetry collector image from
    registry: quay.io
    # The name of the opentelemetry collector image to pull
    name: signalfx/splunk-otel-collector
    # The tag of the opentelemetry collector image to pull
    tag: 0.21.1
    # The policy that specifies when the user wants the opentelemetry collector images to be pulled
    pullPolicy: IfNotPresent


################################################################################
# Extra system configuration
################################################################################

## Limits how many pods may be unavailable due to voluntary disruptions.
## https://kubernetes.io/docs/tasks/run-application/configure-pdb/
podDisruptionBudget: {}
  # Minimum number of pods (as a number or percentage) that must remain available.
  # minAvailable:
  # Maximum number of pods (as a number or percentage) that can be unavailable.
  # maxUnavailable:

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

podSecurityPolicy:
  # Specifies whether Pod Security Policy resources should be created.
  # This should be set to `false` if either:
  # a) Pod Security Policies is not enabled in the cluster, or
  # b) you want to create Pod Security Policy resources by yourself.
  create: false

# Create or use existing secret if name is empty default name is used
secret:
  create: true
  name:

# This default tolerations allow the daemonset to be deployed on master nodes,
# so that we can also collect logs and metrics from those nodes.
tolerations:
  - key: node-role.kubernetes.io/master
    effect: NoSchedule

# Defines which nodes should be selected to deploy the o11y collector daemonset.
nodeSelector: {}
terminationGracePeriodSeconds: 600

# Defines node affinity to restrict deployment of the o11y collector daemonset.
affinity: {}

# Defines priorityClassName to assign a priority class to pods.
priorityClassName: ""


################################################################################
# OpenTelemetry "collector" k8s deployment configuration.
# This is an additional deployment of Open-telemetry collector that can be used
# to pass traces trough it, make k8s metadata enrichment and batching.
# Another use case is to point tracing instrumentation libraries directly to
# the collector endpoint instead of local agents. The collector running in the
# passthrough mode is recommended for large k8s clusters, disabled by default.
################################################################################

otelCollector:
  # Defines if collector deployment is enabled
  # Recommended for large k8s clusters, disabled by default.
  enabled: false

  # Number of collector replicas
  replicaCount: 3

  # The ports exposed by the collector container.
  # Any port can be disabled by setting to null.
  # Any changes should be alligned with service.ports configuraition below.
  ports:
    otlp:
      containerPort: 55680
      protocol: TCP
    jaeger-thrift:
      containerPort: 14268
      protocol: TCP
    jaeger-grpc:
      containerPort: 14250
      protocol: TCP
    zipkin:
      containerPort: 9411
      protocol: TCP
    sapm:
      containerPort: 7276
      protocol: TCP
    signalfx:
      containerPort: 9943
      protocol: TCP
    fluentforward:
      containerPort: 8006
      protocol: TCP
    http-forwarder:
      containerPort: 6060
      protocol: TCP
    # opencensus:
    #   containerPort: 55678
    #   protocol: TCP

  resources:
    limits:
      cpu: 4
      # Memory limit value is used as a source for default memory_limiter configuration
      memory: 8Gi

  # Scheduling configurations
  nodeSelector: {}
  tolerations: []
  affinity: {}

  # Pod configurations
  securityContext: {}
  terminationGracePeriodSeconds: 600
  priorityClassName: ""

  # OpenTelemetry Collector configuration for standalone otel-collector deployment can be overriden in this field.
  # Default configuration defined in config/otel-collector-config.yaml
  # Any additional fields will be merged into the defaults,
  # existing fields can be disabled by setting them to `null`.
  config: {}

################################################################################
# OpenTelemetry service config, used for otel collector deployment.
# Disabled by default
################################################################################

# opentelemetry collector service created only if collector.enabled = true
service:
  # Service type
  type: ClusterIP
  # Service annotations
  annotations: {}
  # Ports exposed by the opentelemetry collector service. Container named ports used.
  ports:
    otlp:
      containerPort: 55680
      targetPort: otlp
      protocol: TCP
    jaeger-thrift:
      containerPort: 14268
      targetPort: jaeger-thrift
      protocol: TCP
    jaeger-grpc:
      containerPort: 14250
      targetPort: jaeger-grpc
      protocol: TCP
    zipkin:
      containerPort: 9411
      targetPort: zipkin
      protocol: TCP
    sapm:
      containerPort: 7276
      targetPort: sapm
      protocol: TCP
    signalfx:
      containerPort: 9943
      targetPort: signalfx
      protocol: TCP
    fluentforward:
      containerPort: 8006
      targetPort: fluentforward
      protocol: TCP
    http-forwarder:
      containerPort: 6060
      targetPort: http-forwarder
      protocol: TCP
    # opencensus:
    #   containerPort: 55678
    #   targetPort: opencensus
    #   protocol: TCP
